{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9e5d30b-9155-4a68-814b-eaf78bf84fd2",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 3: Web APIs & NLP\n",
    "## Truth or Truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5196f0f6-a132-4b03-9d66-79028cddf6f3",
   "metadata": {},
   "source": [
    "# Data Collection\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12670204-c644-4666-8980-5dd271ae96ce",
   "metadata": {},
   "source": [
    "## Table of Contents:\n",
    "- [Data Acquisition](#data_acquisition)\n",
    "- [Data Concatenation](#data_concatenation)\n",
    "- [Next Step](#next_step_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b3991d-2fe2-4615-98cb-ab0d375077e2",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19c247c8-a0ee-4b70-a060-d8954c211492",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457cba79-e9fa-4291-96bb-16559e68f330",
   "metadata": {},
   "source": [
    "## Data Acquisition<a id='data_acquisition'></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3311fd14-9008-4f12-ac70-edb0b08506d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#collect 1000 posts from chosen subreddit within varying timeframes using Pushshift API\n",
    "#json output converted to dataframe and saved as a csv file\n",
    "\n",
    "def get_json_post(sub):\n",
    "    params = {\n",
    "        'size' : '1000', #Collect 1000 posts\n",
    "    }\n",
    "    params[\"subreddit\"] = sub\n",
    "    \n",
    "    #define intervals of 120 days\n",
    "    after_list = ['120d', '240d', '360d', '480d','600d', '720d', '840d', '960d', '1080d', '1200d']\n",
    "    before_list = ['0d', '120d', '240d', '360d', '480d','600d', '720d', '840d', '960d', '1080d']\n",
    "    \n",
    "    url = \" https://api.pushshift.io/reddit/search/submission\"\n",
    "    \n",
    "    #make api call, extract json,convert to dataframe\n",
    "    #save dataframe as csv file if response code is 200\n",
    "    for i in range(len(after_list)):\n",
    "        params[\"since\"] = after_list[i]\n",
    "        params[\"until\"] = before_list[i]\n",
    "        response = requests.get(url,params)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            posts = data['data']\n",
    "            df = pd.DataFrame(posts)\n",
    "            df.to_csv(f\"../data/reddit_{sub}/posts/{after_list[i]}_{before_list[i]}.csv\")\n",
    "            print(f\"Success with {after_list[i]} to {before_list[i]} time frame!!\")\n",
    "            \n",
    "        #print error message for specific time frame\n",
    "        else:\n",
    "            print(f\"Error with {after_list[i]} to {before_list[i]} time frame.\")\n",
    "        \n",
    "        #pause 90 seconds before successive API calls\n",
    "        time.sleep(90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b7912a-e847-40ed-8f0c-a0cac7c763eb",
   "metadata": {},
   "source": [
    "## r/conspiracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fd7703a-58dc-4ed0-a3ff-273c26922945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with 120d to 0d time frame!!\n",
      "Success with 240d to 120d time frame!!\n",
      "Success with 360d to 240d time frame!!\n",
      "Success with 480d to 360d time frame!!\n",
      "Success with 600d to 480d time frame!!\n",
      "Success with 720d to 600d time frame!!\n",
      "Success with 840d to 720d time frame!!\n",
      "Success with 960d to 840d time frame!!\n",
      "Success with 1080d to 960d time frame!!\n",
      "Success with 1200d to 1080d time frame!!\n"
     ]
    }
   ],
   "source": [
    "#collect posts from r/conspiracy\n",
    "get_json_post(\"conspiracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214fd2d-d077-44e3-b62a-e22ae0666833",
   "metadata": {},
   "source": [
    "## r/news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5723620-435f-4240-b933-6ee202a93046",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success with 120d to 0d time frame!!\n",
      "Success with 240d to 120d time frame!!\n",
      "Success with 360d to 240d time frame!!\n",
      "Success with 480d to 360d time frame!!\n",
      "Success with 600d to 480d time frame!!\n",
      "Success with 720d to 600d time frame!!\n",
      "Success with 840d to 720d time frame!!\n",
      "Success with 960d to 840d time frame!!\n",
      "Success with 1080d to 960d time frame!!\n",
      "Success with 1200d to 1080d time frame!!\n"
     ]
    }
   ],
   "source": [
    "#collect posts from r/news\n",
    "get_json_post('news')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c149c8-c677-477f-9857-4a08766eb247",
   "metadata": {},
   "source": [
    "## Data Concatenation<a id='data_concatenation'></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d221610-4da4-4f17-9947-04ee30330298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#concatenate all data in posts files into one dataframe\n",
    "# save as csv\n",
    "def concatenate_posts(files,sub):\n",
    "    \n",
    "    #empty dataframe for concatenation\n",
    "    df_posts = pd.DataFrame()\n",
    "    \n",
    "    #concatenate all posts file into one dataframe\n",
    "    #save only subreddit, post title, and date time variables\n",
    "    for file in files:\n",
    "        if file.endswith(\"csv\"):\n",
    "            df = pd.read_csv(f\"../data/reddit_{sub}/posts/{file}\")\n",
    "            df_posts = pd.concat([df_posts,df[[\"subreddit\",\"title\",\"utc_datetime_str\"]]])\n",
    "    \n",
    "    #convert dataframe to csv\n",
    "    df_posts.to_csv(f\"../data/reddit_{sub}/merged/{sub}_posts.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "73b18768-0d79-4fcf-aa6c-2eee3e8b7b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lists of all r/conspiracy and r/news posts files\n",
    "c_files = os.listdir(\"../data/reddit_conspiracy/posts/\")\n",
    "n_files = os.listdir(\"../data/reddit_news/posts/\")\n",
    "\n",
    "#concatenate all posts files into one dataframe\n",
    "#save complete dataframe as a csv file\n",
    "concatenate_posts(c_files,\"conspiracy\")\n",
    "concatenate_posts(n_files,\"news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46e3246-9864-47be-b2bc-199883580184",
   "metadata": {},
   "source": [
    "## Next Step:<a id='next_step_2'></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4bdad5-cccb-4ef1-ad81-5bb5baed467b",
   "metadata": {},
   "source": [
    "### [Data Cleaning and EDA](./03_Data_Cleaning_and_EDA.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
